
# **YAML-Driven Data Pipeline (DPS)**

A modular, configurable, and dataset-agnostic data cleaning system controlled entirely through a YAML rules file.

This project demonstrates how to design a **mini data-pipeline** without hardcoding logic.
All cleaning stepsâ€”missing value handling, encoding, scaling, column droppingâ€”are defined in `settings.yaml`.
The pipeline reads these rules and applies transformations dynamically.

---

## **ğŸ“ Project Structure**

```
yaml-driven-data-pipeline/
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.yaml          # Cleaning rules (YAML-driven)
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                   # Input CSV files
â”‚   â””â”€â”€ processed/             # Cleaned output files
â”‚
â”œâ”€â”€ artifacts/
â”‚   â””â”€â”€ explore_dataset.txt    # Dataset profile report
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ explore_dataset.py     # Utility to profile dataset
â”‚   â””â”€â”€ data_cleaner.py        # Main rule-based cleaning pipeline
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## **ğŸš€ Features**

### âœ” **1. YAML-Based Rules (No Hardcoding)**

All pipeline logic is configured using:

```
config/settings.yaml
```

Allowing full flexibility without changing Python code.

### âœ” **2. Missing Value Handling**

Supports:

* `median`
* `mode`

### âœ” **3. Column Dropping**

Any unwanted columns can be removed through YAML.

### âœ” **4. Encoding**

Supports:

* **Label Encoding**
* **One-Hot Encoding**

### âœ” **5. Feature Scaling**

Supports:

* **StandardScaler**
* **MinMaxScaler**

### âœ” **6. Dataset Exploration**

The `explore_dataset.py` script automatically generates:

* Dataset shape
* Column data types
* Missing values
* Unique counts
* Numerical stats
* Categorical stats

Saved to:

```
artifacts/explore_dataset.txt
```

---

## **ğŸ“ Example YAML Configuration**

```yaml
missing_values:
  strategy:
    Age: median
    Embarked: mode

drop_columns:
  - Cabin
  - Ticket
  - Name

encoding:
  strategy:
    Sex: label
    Embarked: onehot
    Pclass: onehot

scaling:
  strategy:
    Fare: standard
    Age: standard

target_column: Survived

validation:
  drop_duplicates: true
  enforce_schema: false
```

---

## **âš™ How to Run the Pipeline**

### **1. Place your raw dataset**

```
data/raw/your_file.csv
```

### **2. Generate Dataset Exploration Report**

```bash
python src/explore_dataset.py
```

### **3. Run the Data Cleaning Pipeline**

```bash
python src/data_cleaner.py
```

### Output stored at:

```
data/processed/cleaned_<dataset>.csv
```

---

## **ğŸ“¦ Requirements**

Install all dependencies:

```bash
pip install -r requirements.txt
```

---

## **ğŸ¯ Purpose of the Project**

* To practice real-world data cleaning workflows
* To simulate how production ML pipelines use **config files instead of hardcoded logic**
* To prepare for ML engineer workflows involving:

  * Data validation
  * Feature engineering
  * Pipeline automation
  * Config-driven transformations

---

## **ğŸ“Œ Future Enhancements**

* Add schema validation
* Add logging system
* Add CLI interface
* Add train/validation split support
* Add support for custom transformations
* Add exportable transformation report

---

## **ğŸ‘¤ Author**

**Abdul Rahman Shaikh**


